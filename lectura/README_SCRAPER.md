# ğŸš‡ Sistema Inteligente de Web Scraping - Metro de Madrid

## ğŸ“‹ DescripciÃ³n

Sistema avanzado de web scraping en tiempo real para estaciones del Metro de Madrid que:

- **Carga datos automÃ¡ticamente** al hacer clic en estaciones
- **Guarda informaciÃ³n en cache** para evitar consultas repetidas
- **DiferenciaciÃ³n inteligente** entre datos estÃ¡ticos y dinÃ¡micos
- **ActualizaciÃ³n automÃ¡tica** de informaciÃ³n que cambia frecuentemente
- **IntegraciÃ³n completa** con la aplicaciÃ³n Flask

## ğŸ—ï¸ Arquitectura del Sistema

### Componentes Principales

1. **`IntelligentMetroScraper`** - Clase principal del scraper
2. **`MetroScraperAPI`** - API para integraciÃ³n con Flask
3. **`RealtimeMetroScraper`** - Cliente JavaScript para frontend
4. **Base de datos SQLite** - Cache inteligente
5. **Archivo CSV** - Almacenamiento persistente

### Flujo de Datos

```
Usuario hace clic â†’ JavaScript detecta â†’ API Flask â†’ Verificar Cache â†’ Scraping Web â†’ Guardar Cache â†’ Actualizar CSV â†’ Mostrar datos
```

## ğŸš€ CaracterÃ­sticas

### âœ¨ Funcionalidades Principales

- **Scraping en Tiempo Real**: Carga datos automÃ¡ticamente al interactuar con estaciones
- **Cache Inteligente**: Evita consultas repetidas usando base de datos SQLite
- **DiferenciaciÃ³n de Datos**:
  - **EstÃ¡ticos** (24h TTL): Nombre, accesos, correspondencias, servicios
  - **DinÃ¡micos** (5min TTL): Estado ascensores, Ãºltima actualizaciÃ³n
- **ActualizaciÃ³n AutomÃ¡tica**: Solo actualiza datos que han cambiado
- **Procesamiento AsÃ­ncrono**: No bloquea la interfaz durante el scraping
- **Notificaciones en Tiempo Real**: Feedback visual del estado de las operaciones

### ğŸ“Š Datos ExtraÃ­dos

#### Datos EstÃ¡ticos
- Nombre de la estaciÃ³n
- InformaciÃ³n de accesos (vestÃ­bulo, direcciÃ³n)
- Servicios disponibles
- Zona tarifaria
- Correspondencias con otras lÃ­neas
- Conexiones (CercanÃ­as, autobuses, Metro Ligero)
- Horarios de servicio

#### Datos DinÃ¡micos
- Estado de ascensores (funcionando/averiado/mantenimiento)
- Ãšltima actualizaciÃ³n de informaciÃ³n
- Timestamp de scraping

## ğŸ“ Estructura de Archivos

```
herramientas_csv/
â”œâ”€â”€ intelligent_scraper.py      # Sistema principal de scraping
â”œâ”€â”€ generated_stations_data.py  # Datos fijos de estaciones
â””â”€â”€ ...

static/
â”œâ”€â”€ js/
â”‚   â””â”€â”€ realtime_scraper.js     # Cliente JavaScript
â””â”€â”€ css/
    â””â”€â”€ realtime_scraper.css    # Estilos del sistema

datos_estaciones/
â”œâ”€â”€ scraper_cache.db           # Base de datos SQLite (cache)
â”œâ”€â”€ estaciones_completas.csv   # Datos persistentes
â””â”€â”€ ...

templates/
â””â”€â”€ header.html                # Header con integraciÃ³n del sistema
```

## ğŸ”§ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Dependencias

```bash
pip install requests beautifulsoup4 sqlite3
```

### 2. ConfiguraciÃ³n Inicial

```python
from herramientas_csv.intelligent_scraper import IntelligentMetroScraper

# Inicializar scraper
scraper = IntelligentMetroScraper()

# Configurar TTL personalizado (opcional)
scraper.static_data_ttl = 24 * 60 * 60  # 24 horas
scraper.dynamic_data_ttl = 5 * 60       # 5 minutos
```

### 3. IntegraciÃ³n con Flask

```python
from flask import Flask
from herramientas_csv.intelligent_scraper import IntelligentMetroScraper, MetroScraperAPI

app = Flask(__name__)
scraper = IntelligentMetroScraper()
scraper_api = MetroScraperAPI(scraper)

@app.route('/api/station/<line_number>/<station_id>')
def get_station_data(line_number, station_id):
    return scraper_api.get_station_data(line_number, station_id)
```

## ğŸ¯ Uso del Sistema

### Uso BÃ¡sico

```python
# Obtener datos de una estaciÃ³n
result = scraper.scrape_station_data('2', '255')

if result:
    static_data = result['static']
    dynamic_data = result['dynamic']
    from_cache = result['from_cache']
    
    print(f"EstaciÃ³n: {static_data['name']}")
    print(f"Estado ascensores: {dynamic_data['elevator_status']}")
    print(f"Desde cache: {from_cache}")
```

### Uso Avanzado

```python
# Forzar actualizaciÃ³n
result = scraper.scrape_station_data('2', '255', force_refresh=True)

# ActualizaciÃ³n en lote
stations = [
    {'line_number': '1', 'station_id': '1'},
    {'line_number': '1', 'station_id': '2'},
    {'line_number': '2', 'station_id': '255'}
]
scraper.batch_update_stations(stations)

# Procesamiento asÃ­ncrono
def callback(result):
    print(f"Datos recibidos: {result}")

scraper.request_station_data_async('2', '255', callback)
```

### Uso en JavaScript

```javascript
// El sistema se inicializa automÃ¡ticamente
// Al hacer clic en una estaciÃ³n, se cargan los datos automÃ¡ticamente

// Forzar actualizaciÃ³n manual
window.metroScraper.refreshStation('2', '255');

// Obtener estado del cache
const cacheStatus = window.metroScraper.getCacheStatus();
```

## ğŸ”Œ API Endpoints

### GET `/api/station/<line_number>/<station_id>`
Obtiene datos de una estaciÃ³n (usa cache si estÃ¡ disponible)

**Respuesta:**
```json
{
  "success": true,
  "data": {
    "static": {
      "station_id": "255",
      "line_number": "2",
      "name": "Las Rosas",
      "accesses": [...],
      "services": [...],
      "correspondences": [...]
    },
    "dynamic": {
      "elevator_status": "Funcionando",
      "last_update": "Actualizado hace 5 minutos",
      "scraped_at": "2024-01-15T10:30:00"
    }
  },
  "from_cache": true,
  "timestamp": "2024-01-15T10:30:00"
}
```

### GET `/api/station/<line_number>/<station_id>/refresh`
Fuerza la actualizaciÃ³n de datos de una estaciÃ³n

### GET `/api/cache/status`
Obtiene estadÃ­sticas del cache

**Respuesta:**
```json
{
  "success": true,
  "cache_stats": {
    "total_cached_stations": 45,
    "recently_updated": 12,
    "cache_file": "datos_estaciones/scraper_cache.db",
    "csv_file": "datos_estaciones/estaciones_completas.csv"
  }
}
```

## ğŸ§ª Pruebas

### Ejecutar Pruebas

```bash
python test_scraper.py
```

### Pruebas Incluidas

- âœ… Scraping de estaciones individuales
- âœ… VerificaciÃ³n de cache
- âœ… ActualizaciÃ³n forzada
- âœ… GeneraciÃ³n de CSV
- âœ… EstadÃ­sticas del sistema

## ğŸ“ˆ Monitoreo y EstadÃ­sticas

### MÃ©tricas Disponibles

- **Estaciones en cache**: Total de estaciones almacenadas
- **Actualizaciones recientes**: Estaciones actualizadas en la Ãºltima hora
- **Tiempo de respuesta**: Velocidad de las consultas
- **Tasa de Ã©xito**: Porcentaje de scraping exitoso

### Logs del Sistema

```
ğŸ” Scraping estaciÃ³n 255 de lÃ­nea 2
âœ… Datos obtenidos del cache para estaciÃ³n 255
âœ… Datos actualizados para estaciÃ³n 255
ğŸ“Š Datos cargados (desde cache)
```

## ğŸ”’ Consideraciones de Seguridad

### Rate Limiting
- Pausa de 1 segundo entre consultas en lote
- Timeout de 10 segundos por consulta
- User-Agent personalizado para evitar bloqueos

### Manejo de Errores
- Reintentos automÃ¡ticos en caso de fallo
- Fallback a datos del cache
- Notificaciones de error en tiempo real

## ğŸš€ Optimizaciones

### Performance
- **Cache inteligente**: Evita consultas innecesarias
- **Procesamiento asÃ­ncrono**: No bloquea la interfaz
- **CompresiÃ³n de datos**: Almacenamiento eficiente
- **Lazy loading**: Carga datos solo cuando se necesitan

### Escalabilidad
- **Base de datos SQLite**: Ligera y eficiente
- **Cola de procesamiento**: Manejo de mÃºltiples requests
- **TTL configurable**: Control de frescura de datos

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Personalizar TTL

```python
scraper = IntelligentMetroScraper()

# Datos estÃ¡ticos (nombres, accesos, etc.)
scraper.static_data_ttl = 24 * 60 * 60  # 24 horas

# Datos dinÃ¡micos (estado ascensores, etc.)
scraper.dynamic_data_ttl = 5 * 60       # 5 minutos
```

### Configurar User-Agent

```python
scraper.session.headers.update({
    'User-Agent': 'Tu-Aplicacion/1.0 (contacto@tuapp.com)'
})
```

### Personalizar URLs

```python
def custom_url_generator(line_number, station_id):
    return f"https://tu-dominio.com/linea/{line_number}/estacion/{station_id}"

scraper.get_station_url = custom_url_generator
```

## ğŸ› SoluciÃ³n de Problemas

### Problemas Comunes

1. **Error de conexiÃ³n**
   - Verificar conectividad a internet
   - Comprobar que la web de Metro Madrid estÃ© accesible

2. **Datos no se actualizan**
   - Verificar TTL configurado
   - Forzar actualizaciÃ³n manual

3. **Cache corrupto**
   - Eliminar archivo `scraper_cache.db`
   - Reiniciar aplicaciÃ³n

4. **JavaScript no funciona**
   - Verificar que `realtime_scraper.js` estÃ© cargado
   - Comprobar consola del navegador

### Debug Mode

```python
import logging
logging.basicConfig(level=logging.DEBUG)

scraper = IntelligentMetroScraper()
# Ahora verÃ¡s logs detallados
```

## ğŸ“ Soporte

Para problemas o preguntas sobre el sistema de scraping:

1. Revisar logs de la aplicaciÃ³n
2. Ejecutar `test_scraper.py` para diagnÃ³stico
3. Verificar estado del cache en `/api/cache/status`

## ğŸ”„ Actualizaciones Futuras

- [ ] Soporte para mÃºltiples fuentes de datos
- [ ] Machine Learning para predicciÃ³n de cambios
- [ ] API REST completa
- [ ] Dashboard de administraciÃ³n
- [ ] ExportaciÃ³n a mÃºltiples formatos
- [ ] IntegraciÃ³n con APIs oficiales

---

**Â¡El sistema estÃ¡ listo para usar! ğŸš‡âœ¨** 